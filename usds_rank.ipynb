{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading necessary Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the necessary packages for processing to generate the wordcloud header and inputs for the visualizations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, json, re, numpy as np, fiona, sys, matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from shapely.geometry import asShape\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "from scipy.misc import imread\n",
    "from PIL import Image\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to process ACS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a basic function to load CSVs or JSONs. This is simply for ease of coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadinput(fname, tp):\n",
    "    with open(fname) as data:\n",
    "        if tp == 'csv':\n",
    "            loadme = []\n",
    "            csvloadme = csv.reader(data)\n",
    "            for row in csvloadme:\n",
    "                loadme.append(row)\n",
    "        elif tp == 'json':\n",
    "            loadme = json.load(data)\n",
    "        elif tp == 'jsons':\n",
    "            loadme = json.loads(data)\n",
    "        else:\n",
    "            print 'Need defined filetype'\n",
    "    return loadme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define how to process a given ACS estimate file and combine it with its associated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_acs_sum(fname):\n",
    "    # print 'Extracting: %s'%fname\n",
    "    typ = fname[9:10]\n",
    "    st_nm = fname[15:17]\n",
    "    seq_nm = fname[17:21]\n",
    "    acs = []\n",
    "    with open(fname, 'rb') as f:\n",
    "        for row in csv.reader(f, delimiter=','):\n",
    "            acs.append(row)\n",
    "    # Get Header info\n",
    "    with open('./2013_5yr_SAS/%s%s_%s.sas'%(typ,st_nm,seq_nm)) as f:\n",
    "        read_flg = False\n",
    "        header = []\n",
    "        for ln in f:\n",
    "            if ln.strip() not in ('', ';', 'RUN;') and read_flg:\n",
    "                header.append(re.split('\\s*', ln.strip())[0])\n",
    "            read_flg = True if 'INPUT' in ln else read_flg\n",
    "    # Get geography\n",
    "    with open('./tab4/sumfile/prod/2009thru2013/geo/g20135%s.txt'%st_nm,'rb') as f:\n",
    "        geog = {}\n",
    "        nottract = []\n",
    "        for row in f:\n",
    "            logrecno = row[13:20]\n",
    "            geoid = row[178:218].strip()\n",
    "            gname = row[218:].strip().decode('utf-8', 'ignore')\n",
    "            geog[logrecno] = [geoid, gname]\n",
    "        for i, row in enumerate(acs):\n",
    "            if geog[row[5]][0][0:2] == '14':\n",
    "                row.extend(geog[row[5]])\n",
    "            else:\n",
    "                # Remove Non tract geography\n",
    "                nottract.append(i)\n",
    "        # Reverse sort to delete without messing index\n",
    "        nottract.sort(reverse=True)\n",
    "        for row in nottract:\n",
    "            del acs[row]\n",
    "    return [header + ['geoid', 'gname']] + acs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to parse the directory with the ACS estimates file since they are broken up by state and output it to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_directory(dirnm, force=False):\n",
    "    if force or not os.path.isfile('directory.json'):\n",
    "        directory = {}\n",
    "        for fname in tqdm(os.listdir(dirnm)):\n",
    "            typ = fname[0:1]\n",
    "            st_nm = fname[1:3]\n",
    "            seq_nm = fname[4:8]\n",
    "            with open(dirnm + fname, 'rb') as f:\n",
    "                read_flg = False\n",
    "                for ln in f:\n",
    "                    if ln.strip() not in ('', ';', 'RUN;') and read_flg:\n",
    "                        varnm = re.split('\\s*', ln.strip())[0]\n",
    "                        try:\n",
    "                            directory[varnm].append([typ, st_nm, seq_nm])\n",
    "                        except KeyError:\n",
    "                            directory[varnm] = [[typ, st_nm, seq_nm]]\n",
    "                    read_flg = True if 'INPUT' in ln else read_flg\n",
    "        for key in ['FILEID', 'FILETYPE', 'STUSAB', 'CHARITER', 'SEQUENCE', 'LOGRECNO']:\n",
    "            directory.pop(key)\n",
    "        with open('directory.json', 'wb') as f:\n",
    "            json.dump(directory, f)\n",
    "    with open('directory.json', 'rb') as f:\n",
    "        directory = json.load(f)\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function to fix the slight naming discrepancy between the ACS Variable Inventory JSON and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_varlist(fname):\n",
    "    print 'Loading ACS 5 year Summary File Variable List'\n",
    "    with open(fname, 'r') as f:\n",
    "        varlist = json.load(f)['variables']\n",
    "    for varname in tqdm(varlist.keys()):\n",
    "        if '0' in varname:\n",
    "            split = re.split('_', varname)\n",
    "            corrected = split[0] + split[1][-1].lower() + split[1].strip('0')[0:-1]\n",
    "            varlist[corrected] = varlist.pop(varname)\n",
    "    return varlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we put it all together by processing through each state pulling only the variables we want and output to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_custom_json(varlist, score, force=False):\n",
    "    with open('header.json') as f:\n",
    "        existing_score = json.load(f).keys()\n",
    "    with open('header.json') as f:\n",
    "        existing_variables = json.load(f).keys()\n",
    "    if score != existing_score or varlist != existing_variables:\n",
    "        force = True\n",
    "    if force or not os.path.isfile('custom.json'):\n",
    "        directory = build_directory('./2013_5yr_SAS/')\n",
    "        custom = {}\n",
    "        header = []\n",
    "        for var in tqdm(varlist):\n",
    "            header.extend([var])\n",
    "            if var in score:\n",
    "                header.extend(['score_' + var])\n",
    "            for table in directory[var]:\n",
    "                typ = table[0]\n",
    "                st_nm = table[1]\n",
    "                seq_nm = table[2]\n",
    "                acs_summary = extract_acs_sum('./group2/' + typ + '20135' + st_nm + seq_nm + '000.txt')\n",
    "                position = [ i for i,x in enumerate(acs_summary[0]) if x == var ]\n",
    "                if var in score:\n",
    "                    scoreme = [ x[position[0]] for x in acs_summary[1:] ]\n",
    "                    scored = [ round(stats.percentileofscore(scoreme, x),2) for x in scoreme ]\n",
    "                for i, row in enumerate(acs_summary[1:]):\n",
    "                    try:\n",
    "                        custom[row[-2][7:]][var] = row[position[0]]\n",
    "                    except KeyError:\n",
    "                        custom[row[-2][7:]] = {}\n",
    "                        custom[row[-2][7:]][var] = row[position[0]]\n",
    "                    if var in score:\n",
    "                        custom[row[-2][7:]]['score_'+var] = scored[i]\n",
    "                        custom[row[-2][7:]]['gname'] = row[-1]\n",
    "        position = [ x for x in header if 'score' in x ]\n",
    "        for tract in custom.keys():\n",
    "            avg = round(sum([ float(custom[tract][x]) for x in position ])/len(position),2)\n",
    "            custom[tract]['final_score'] = avg\n",
    "        header.extend(['final_score'])\n",
    "        # custom['varnames'] = header\n",
    "        with open('custom.json', 'wb') as f:\n",
    "            json.dump(custom, f)\n",
    "    with open('custom.json', 'rb') as f:\n",
    "        custom = json.load(f)\n",
    "    return custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing CFPB Financial Complaints Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now process CFPB Fiancial Complaints Database. This involves loading the downladed CSV file, counting at the zipcode level and apportioning it to the Census tract level. We write out a JSON of the distribution of financial complaints to see what the upper limit is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complaints = loadinput('./Consumer_Complaints.csv','csv')\n",
    "compl_list = []\n",
    "for complaint in tqdm(complaints[1:]):\n",
    "    compl_list.append(complaint[9])\n",
    "compl_cnt = {}\n",
    "for zips in tqdm(compl_list):\n",
    "    if 'X' not in zips:\n",
    "        if compl_cnt.has_key(zips):\n",
    "            compl_cnt[zips] += 1\n",
    "        else:\n",
    "            compl_cnt[zips] = 1\n",
    "ziptract = [ [(x[0],x[4]),float(x[18])/100] for x in loadinput('./zcta_tract_rel_10.txt', 'csv')[1:] ]\n",
    "for i, row in tqdm(enumerate(ziptract)):\n",
    "    zipcode = row[0][0]\n",
    "    if compl_cnt.has_key(zipcode):\n",
    "        ziptract[i].append(row[1]*compl_cnt[zipcode])\n",
    "    else:\n",
    "        ziptract[i].append(0)\n",
    "compl_append = {}\n",
    "for row in tqdm(ziptract):\n",
    "    tract = row[0][1]\n",
    "    count = round(row[2],0)\n",
    "    if compl_append.has_key(tract) == False:\n",
    "        compl_append[tract] = count\n",
    "    else:\n",
    "        compl_append[tract] += count\n",
    "compl_cnt = [ compl_append[tract] for tract in compl_append.keys() ]\n",
    "compl_dist = [ {'value':x, 'count':compl_cnt.count(x)} for x in set(compl_cnt) ]\n",
    "with open('./vis/dist.json', 'wb') as f:\n",
    "    json.dump(compl_dist, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating GeoJSON files with appended ACS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our variables of interest and build our custom JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables = ['B01001e27', 'B07001e17', 'B22002e2']\n",
    "scores = variables\n",
    "varlist = load_varlist('./variables.json')\n",
    "header = { x: varlist[x] for x in variables }\n",
    "with open('./header.json', 'wb') as f:\n",
    "    json.dump(header, f)\n",
    "scored = { x: varlist[x] for x in scores }\n",
    "with open('./scored.json', 'wb') as f:\n",
    "    json.dump(scored, f)\n",
    "custom = build_custom_json(variables, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have a directory of shapefiles that we want to append the additional information from the JSON file to the geography. We mount a virtual drive to save disk space for each archive and write the resulting GeoJSON for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootdir = './tracts/'\n",
    "us_features = []\n",
    "empty_tracts = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for f in tqdm(files):\n",
    "        if f[-3:] == 'zip':\n",
    "            features = []\n",
    "            with fiona.open('/', vfs='zip://'+os.path.join(subdir, f), layer=0) as src:\n",
    "                for feat in src:\n",
    "                    feat['properties']['x'] = round(asShape(feat['geometry']).centroid.x,4)\n",
    "                    feat['properties']['y'] = round(asShape(feat['geometry']).centroid.y,4)\n",
    "                    tract = feat['properties']['GEO_ID'][9:]\n",
    "                    try:\n",
    "                        feat['properties']['g'] = custom[tract]['gname']                           \n",
    "                        for popme in [u'NAME', u'LSAD', u'STATE', u'COUNTY', u'TRACT', u'CENSUSAREA', u'GEO_ID']:\n",
    "                            feat['properties'].pop(popme)\n",
    "                        for rank in variables:\n",
    "                            feat['properties']['s'+rank] = custom[tract]['score_'+rank]\n",
    "                        feat['properties']['r'] = custom[tract]['final_score']\n",
    "                        try:\n",
    "                            feat['properties']['f'] = round(compl_append[tract],0)\n",
    "                        except KeyError:\n",
    "                            feat['properties']['f'] = 0\n",
    "                        features.append(feat)\n",
    "                        us_features.append(feat)\n",
    "                    except KeyError:\n",
    "                        empty_tracts.append(tract)\n",
    "            rank_map = {\n",
    "                'type':'FeatureCollection',\n",
    "                'features':features,\n",
    "                'crs':{'init': u'epsg:4269'}}\n",
    "            with open('./tracts/%s.geojson'%f, 'wb') as fl:\n",
    "                json.dump(rank_map, fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do another pass and throwout geography of no interest to save space. We then write out the entire nation to a GeoJSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat in tqdm(us_features):\n",
    "    for rank in variables:\n",
    "        feat['properties'].pop('s'+rank)\n",
    "    if feat['properties']['r'] < 70 or feat['properties']['f'] < 10:\n",
    "        us_features.remove(feat)\n",
    "rank_map = {\n",
    "    'type':'FeatureCollection',\n",
    "    'features':us_features,\n",
    "    'crs':{'init': u'epsg:4269'}}\n",
    "with open('./tracts/us_all_tracts.geojson', 'wb') as f:\n",
    "    json.dump(rank_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing a Word Cloud for ACS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the wordcloud package in Python to generate a wordcloud from the descriptions in ACS Variable Inventory. Using the original image, we color shift the white to another color and then overlay a color mask to get the final image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flag = Image.open('./img/America.png').convert('RGBA')\n",
    "im = np.array(flag)\n",
    "imcolor = im\n",
    "red, green, blue, alpha = im.T\n",
    "white_areas = (red == 255) & (green == 255) & (blue == 255)\n",
    "null_areas = (red == 0) & (green == 0) & (blue == 0) & (alpha == 0)\n",
    "im[..., :-1][white_areas.T] = (255, 0, 0)\n",
    "im[..., :-1][null_areas.T] = (255, 255, 255)\n",
    "immask = im\n",
    "Image.fromarray(im).save('./img/America-shifted.png')\n",
    "varlist = load_varlist('./variables.json')\n",
    "text = ''\n",
    "for var in tqdm(varlist.keys()):\n",
    "    text += varlist[var][u'concept']\n",
    "stopwords = STOPWORDS.copy()\n",
    "stopwords.add(\"months\")\n",
    "stopwords.add(\"year\")\n",
    "stopwords.add(\"years\")\n",
    "stopwords.add(\"past\")\n",
    "wordcloud = WordCloud(background_color=\"white\", width=2560, height=1440, scale=1, stopwords=stopwords, mask=immask).generate(text)\n",
    "image_colors = ImageColorGenerator(imcolor)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors))\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('./img/word_cloud.png', dpi=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
